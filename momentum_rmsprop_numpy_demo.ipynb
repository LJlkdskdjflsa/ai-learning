{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "08280f3a",
   "metadata": {},
   "source": [
    "# Gradient Descent with Momentum & RMSprop (NumPy)\n",
    "\n",
    "This notebook demonstrates **gradient descent**, **gradient descent with momentum**, and **RMSprop** using pure NumPy on a simple 2D optimization problem.\n",
    "\n",
    "內容大綱：\n",
    "\n",
    "1. 定義一個簡單但「不等方」的二次函數（讓震盪現象明顯）  \n",
    "2. 實作：\n",
    "   - 標準梯度下降 (Vanilla Gradient Descent)\n",
    "   - 帶動量的梯度下降 (Momentum)\n",
    "   - RMSprop\n",
    "3. 比較不同演算法的收斂軌跡與效果\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f4009f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For nicer printing\n",
    "np.set_printoptions(precision=4, suppress=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e2f82d",
   "metadata": {},
   "source": [
    "## 1. Toy Objective Function\n",
    "\n",
    "我們使用一個簡單的 2D 二次函數：\n",
    "\n",
    "\\[\n",
    "f(w_1, w_2) = a w_1^2 + b w_2^2\n",
    "\\]\n",
    "\n",
    "其中：\n",
    "\n",
    "- 若 \\(a \\ll b\\)，則在 \\(w_1\\) 方向較平，在 \\(w_2\\) 方向較陡  \n",
    "- 這會導致普通梯度下降在陡峭方向劇烈震盪，在平坦方向移動很慢 → 非常適合展示 Momentum 和 RMSprop 的效果。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e48055c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quadratic function parameters (ill-conditioned)\n",
    "a = 0.1  # flatter direction\n",
    "b = 5.0  # steeper direction\n",
    "\n",
    "def f(w):\n",
    "    \"\"\"Objective function: f(w1, w2) = a*w1^2 + b*w2^2\"\"\"\n",
    "    w1, w2 = w\n",
    "    return a * w1**2 + b * w2**2\n",
    "\n",
    "def grad_f(w):\n",
    "    \"\"\"Gradient of f with respect to w = [w1, w2].\"\"\"\n",
    "    w1, w2 = w\n",
    "    df_dw1 = 2 * a * w1\n",
    "    df_dw2 = 2 * b * w2\n",
    "    return np.array([df_dw1, df_dw2])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f094978f",
   "metadata": {},
   "source": [
    "## 2. 可視化工具：等高線與軌跡\n",
    "\n",
    "我們定義一個輔助函式，用來在 2D 平面畫出：\n",
    "\n",
    "- 目標函數的等高線 (contours)  \n",
    "- 優化過程中每一步的參數位置（軌跡）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c2ff098",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_contours_with_trajectory(trajectory, title=\"Trajectory\", levels=30):\n",
    "    trajectory = np.array(trajectory)\n",
    "    w1_vals = trajectory[:, 0]\n",
    "    w2_vals = trajectory[:, 1]\n",
    "\n",
    "    # 建立網格\n",
    "    w1_min, w1_max = w1_vals.min() - 1.0, w1_vals.max() + 1.0\n",
    "    w2_min, w2_max = w2_vals.min() - 1.0, w2_vals.max() + 1.0\n",
    "\n",
    "    W1, W2 = np.meshgrid(\n",
    "        np.linspace(w1_min, w1_max, 200),\n",
    "        np.linspace(w2_min, w2_max, 200)\n",
    "    )\n",
    "    Z = a * W1**2 + b * W2**2\n",
    "\n",
    "    plt.figure(figsize=(6, 5))\n",
    "    plt.contour(W1, W2, Z, levels=levels)\n",
    "    plt.plot(w1_vals, w2_vals, marker=\"o\")\n",
    "    plt.scatter([0], [0])\n",
    "    plt.title(title)\n",
    "    plt.xlabel(\"w1\")\n",
    "    plt.ylabel(\"w2\")\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c80c35",
   "metadata": {},
   "source": [
    "## 3. 標準梯度下降 (Vanilla Gradient Descent)\n",
    "\n",
    "更新規則：\n",
    "\n",
    "\\[\n",
    "w := w - \\alpha \\, \\nabla f(w)\n",
    "\\]\n",
    "\n",
    "其中：\n",
    "\n",
    "- \\(\\alpha\\) 是學習率 (learning rate)\n",
    "- \\(\\nabla f(w)\\) 是梯度\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060b9654",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent(w_init, alpha=0.05, num_iters=50):\n",
    "    w = w_init.copy().astype(float)\n",
    "    trajectory = [w.copy()]\n",
    "    costs = [f(w)]\n",
    "    for i in range(num_iters):\n",
    "        grad = grad_f(w)\n",
    "        w = w - alpha * grad\n",
    "        trajectory.append(w.copy())\n",
    "        costs.append(f(w))\n",
    "    return np.array(trajectory), np.array(costs)\n",
    "\n",
    "# Run vanilla GD from a starting point\n",
    "w0 = np.array([3.0, 3.0])  # initial point\n",
    "traj_gd, costs_gd = gradient_descent(w0, alpha=0.05, num_iters=50)\n",
    "print(\"Final w (GD):\", traj_gd[-1])\n",
    "print(\"Final cost (GD):\", costs_gd[-1])\n",
    "\n",
    "plot_contours_with_trajectory(traj_gd, title=\"Vanilla Gradient Descent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f13d739",
   "metadata": {},
   "source": [
    "## 4. 帶動量的梯度下降 (Momentum)\n",
    "\n",
    "Momentum 的更新規則如下（使用指數加權平均）：\n",
    "\n",
    "1. 動量更新：\n",
    "\n",
    "\\[\n",
    "v := \\beta v + (1 - \\beta) \\, \\nabla f(w)\n",
    "\\]\n",
    "\n",
    "2. 參數更新：\n",
    "\n",
    "\\[\n",
    "w := w - \\alpha v\n",
    "\\]\n",
    "\n",
    "其中：\n",
    "\n",
    "- \\(v\\)：梯度的指數加權平均（類似「速度」）  \n",
    "- \\(\\beta\\)：動量超參數（常用 0.9）  \n",
    "- \\(\\alpha\\)：學習率  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "333f09dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gradient_descent_momentum(w_init, alpha=0.05, beta=0.9, num_iters=50):\n",
    "    w = w_init.copy().astype(float)\n",
    "    v = np.zeros_like(w)\n",
    "    trajectory = [w.copy()]\n",
    "    costs = [f(w)]\n",
    "    for i in range(num_iters):\n",
    "        grad = grad_f(w)\n",
    "        v = beta * v + (1 - beta) * grad\n",
    "        w = w - alpha * v\n",
    "        trajectory.append(w.copy())\n",
    "        costs.append(f(w))\n",
    "    return np.array(trajectory), np.array(costs)\n",
    "\n",
    "traj_mom, costs_mom = gradient_descent_momentum(w0, alpha=0.1, beta=0.9, num_iters=50)\n",
    "print(\"Final w (Momentum):\", traj_mom[-1])\n",
    "print(\"Final cost (Momentum):\", costs_mom[-1])\n",
    "\n",
    "plot_contours_with_trajectory(traj_mom, title=\"Gradient Descent with Momentum\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9473be47",
   "metadata": {},
   "source": [
    "## 5. RMSprop\n",
    "\n",
    "RMSprop 會對「梯度平方」做指數加權平均，並用來調整每一個維度的「有效步長」。\n",
    "\n",
    "更新規則：\n",
    "\n",
    "1. 累積平方梯度的指數加權平均：\n",
    "\n",
    "\\[\n",
    "S := \\beta_2 S + (1 - \\beta_2) (\\nabla f(w))^2\n",
    "\\]\n",
    "\n",
    "> 上式中的平方與加法都是 **逐元素 (element-wise)**。\n",
    "\n",
    "2. 參數更新：\n",
    "\n",
    "\\[\n",
    "w := w - \\alpha \\frac{\\nabla f(w)}{\\sqrt{S} + \\varepsilon}\n",
    "\\]\n",
    "\n",
    "- \\(\\beta_2\\)：RMSprop 的衰減係數（常用 0.9、0.99 等）  \n",
    "- \\(\\varepsilon\\)：一個很小的常數，例如 \\(10^{-8}\\)，避免除以 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ac9c517",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rmsprop(w_init, alpha=0.05, beta2=0.9, epsilon=1e-8, num_iters=50):\n",
    "    w = w_init.copy().astype(float)\n",
    "    S = np.zeros_like(w)\n",
    "    trajectory = [w.copy()]\n",
    "    costs = [f(w)]\n",
    "    for i in range(num_iters):\n",
    "        grad = grad_f(w)\n",
    "        # Exponentially weighted average of squared gradients\n",
    "        S = beta2 * S + (1 - beta2) * (grad ** 2)\n",
    "        # Parameter update (element-wise division)\n",
    "        w = w - alpha * grad / (np.sqrt(S) + epsilon)\n",
    "        trajectory.append(w.copy())\n",
    "        costs.append(f(w))\n",
    "    return np.array(trajectory), np.array(costs)\n",
    "\n",
    "traj_rms, costs_rms = rmsprop(w0, alpha=0.1, beta2=0.9, epsilon=1e-8, num_iters=50)\n",
    "print(\"Final w (RMSprop):\", traj_rms[-1])\n",
    "print(\"Final cost (RMSprop):\", costs_rms[-1])\n",
    "\n",
    "plot_contours_with_trajectory(traj_rms, title=\"RMSprop\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136d78bb",
   "metadata": {},
   "source": [
    "## 6. 收斂速度比較\n",
    "\n",
    "我們將三種方法在同樣起點、類似的學習率設定下的 **loss 曲線** 畫在一起，觀察：\n",
    "\n",
    "- 哪個下降得比較快  \n",
    "- 哪個震盪比較少  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead4af91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用相同起點重新跑，便於比較\n",
    "w0 = np.array([3.0, 3.0])\n",
    "\n",
    "traj_gd, costs_gd = gradient_descent(w0, alpha=0.05, num_iters=80)\n",
    "traj_mom, costs_mom = gradient_descent_momentum(w0, alpha=0.1, beta=0.9, num_iters=80)\n",
    "traj_rms, costs_rms = rmsprop(w0, alpha=0.1, beta2=0.9, epsilon=1e-8, num_iters=80)\n",
    "\n",
    "plt.figure(figsize=(7, 5))\n",
    "plt.plot(costs_gd, label=\"GD\")\n",
    "plt.plot(costs_mom, label=\"Momentum\")\n",
    "plt.plot(costs_rms, label=\"RMSprop\")\n",
    "plt.yscale(\"log\")  # 常用 log scale 觀察收斂速度\n",
    "plt.xlabel(\"Iteration\")\n",
    "plt.ylabel(\"Cost (log scale)\")\n",
    "plt.title(\"Convergence Comparison\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "080692af",
   "metadata": {},
   "source": [
    "## 7. 小結\n",
    "\n",
    "- **Vanilla Gradient Descent**：\n",
    "  - 同一學習率作用在所有維度上  \n",
    "  - 在「陡峭方向」容易震盪，為了避免發散，必須用很小的學習率 → 收斂變慢  \n",
    "\n",
    "- **Momentum**：\n",
    "  - 對梯度做指數加權平均，像是讓參數有「速度」  \n",
    "  - 沿著一致方向會越滾越快，在來回震盪方向會互相抵消  \n",
    "  - 通常能明顯加快收斂，減少 Zig-Zag 路徑  \n",
    "\n",
    "- **RMSprop**：\n",
    "  - 對「梯度平方」做指數加權平均  \n",
    "  - 在梯度一直很大的方向，自動縮小步伐；在梯度小的方向，相對放大步伐  \n",
    "  - 等同於為每個參數維度設定「自適應學習率」  \n",
    "\n",
    "後續若再結合 Momentum 與 RMSprop，就可以得到常用的 **Adam** 演算法。"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
